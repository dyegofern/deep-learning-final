{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "078ba63e",
        "outputId": "75ec8f7b-14d5-4255-f2ee-c9b86b2e7391"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the folders to be mounted\n",
        "folders_to_mount = ['src', 'data', 'notebooks', 'plots']\n",
        "\n",
        "# Define the target directory in /content/\n",
        "content_dir = '/content/'\n",
        "\n",
        "for folder in folders_to_mount:\n",
        "    source_path = os.path.join('..', folder) # Assuming folders are in the parent directory\n",
        "    target_path = os.path.join(content_dir, folder)\n",
        "\n",
        "    if not os.path.exists(target_path):\n",
        "        try:\n",
        "            os.symlink(source_path, target_path)\n",
        "            print(f'Symlinked {source_path} to {target_path}')\n",
        "        except Exception as e:\n",
        "            print(f'Error creating symlink for {folder}: {e}')\n",
        "    else:\n",
        "        print(f'{target_path} already exists, skipping symlink creation.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symlinked ../src to /content/src\n",
            "Symlinked ../data to /content/data\n",
            "Symlinked ../notebooks to /content/notebooks\n",
            "Symlinked ../plots to /content/plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv3VhUXGF-uS"
      },
      "source": [
        "# Improved CTGAN Training: Enhanced Synthetic Data Generation\n",
        "\n",
        "## Key Improvements\n",
        "\n",
        "1. **Proper handling of mixed data types**\n",
        "   - Continuous features: Mode-specific normalization\n",
        "   - Binary features: Preserved as 0/1\n",
        "   - One-hot encoded: Gumbel-softmax for differentiability\n",
        "\n",
        "2. **Better architecture**\n",
        "   - Deeper networks (512x512x512)\n",
        "   - Separate output heads for different data types\n",
        "   - Improved dropout and batch normalization\n",
        "\n",
        "3. **Enhanced training**\n",
        "   - More epochs (300 vs 100)\n",
        "   - Early stopping based on Wasserstein distance\n",
        "   - Better monitoring and validation\n",
        "\n",
        "4. **Comprehensive validation**\n",
        "   - Type-specific quality metrics\n",
        "   - One-hot validity checking\n",
        "   - Distribution alignment metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P49rkH58F-uV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "__qmjHKbF-uW",
        "outputId": "5a30b19e-75fa-4048-8fb4-ac71d5a19a12"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-844784619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Import improved modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimproved_ctgan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_improved_ctgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimproved_training\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_improved_ctgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_synthetic_quality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_validation_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import improved modules\n",
        "from src.improved_ctgan import build_improved_ctgan, DataTransformer\n",
        "from src.improved_training import train_improved_ctgan, validate_synthetic_quality, print_validation_report\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f'TensorFlow: {tf.__version__}')\n",
        "print('Improved CTGAN modules loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuNDsyrsF-uY"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sylnIObtF-uY"
      },
      "outputs": [],
      "source": [
        "# Load unscaled training data\n",
        "train_data = pd.read_csv('../data/processed/train_data_unscaled.csv')\n",
        "\n",
        "print(f'Training data shape: {train_data.shape}')\n",
        "print(f'\\nColumn names:')\n",
        "print(list(train_data.columns))\n",
        "print(f'\\nFirst few rows:')\n",
        "print(train_data.head())\n",
        "print(f'\\nData types:')\n",
        "print(train_data.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05ctmQ5F-uZ"
      },
      "source": [
        "## 2. Build Improved CTGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsXGhMbNF-ua"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy array\n",
        "train_array = train_data.values.astype(np.float32)\n",
        "column_names = list(train_data.columns)\n",
        "\n",
        "# Build improved CTGAN with data transformer\n",
        "improved_ctgan, data_transformer = build_improved_ctgan(\n",
        "    data=train_array,\n",
        "    column_names=column_names,\n",
        "    noise_dim=128,\n",
        "    generator_lr=2e-4,\n",
        "    discriminator_lr=2e-4\n",
        ")\n",
        "\n",
        "print('\\n✓ Improved CTGAN initialized!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRfLt7bJF-uc"
      },
      "source": [
        "## 3. Transform Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzesjcMuF-uc"
      },
      "outputs": [],
      "source": [
        "# Transform data for training (already done during build)\n",
        "transformed_data = data_transformer.transform(train_array)\n",
        "\n",
        "print(f'Transformed data shape: {transformed_data.shape}')\n",
        "print(f'Transformed data range: [{transformed_data.min():.4f}, {transformed_data.max():.4f}]')\n",
        "print(f'\\nTransformation complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMD4zI3EF-ud"
      },
      "source": [
        "## 4. Train Improved CTGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItG3BfI8F-ue"
      },
      "outputs": [],
      "source": [
        "# Train with improved training loop\n",
        "print('Training Improved CTGAN...')\n",
        "print('This will take longer but produce much better results!')\n",
        "print('Expected training time: 5-10 minutes\\n')\n",
        "\n",
        "history = train_improved_ctgan(\n",
        "    ctgan=improved_ctgan,\n",
        "    real_data=transformed_data,\n",
        "    epochs=300,\n",
        "    batch_size=500,\n",
        "    n_critic=5,\n",
        "    verbose=True,\n",
        "    early_stopping_patience=50,\n",
        "    validation_interval=10\n",
        ")\n",
        "\n",
        "print('\\n✓ Training completed!')\n",
        "print(f'Best epoch: {history[\"best_epoch\"]}')\n",
        "print(f'Final G loss: {history[\"g_loss\"][-1]:.4f}')\n",
        "print(f'Final D loss: {history[\"d_loss\"][-1]:.4f}')\n",
        "print(f'Final W distance: {history[\"w_distance\"][-1]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsBhyT2KF-ue"
      },
      "source": [
        "## 5. Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ4lDr-0F-ue"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Generator loss\n",
        "axes[0, 0].plot(history['g_loss'], linewidth=2, color='blue', alpha=0.8)\n",
        "axes[0, 0].axvline(history['best_epoch'], color='red', linestyle='--', label='Best Epoch')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Generator Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title('Generator Loss Over Time', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Discriminator loss\n",
        "axes[0, 1].plot(history['d_loss'], linewidth=2, color='red', alpha=0.8)\n",
        "axes[0, 1].axvline(history['best_epoch'], color='red', linestyle='--', label='Best Epoch')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Discriminator Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('Discriminator Loss Over Time', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Wasserstein distance\n",
        "axes[1, 0].plot(history['w_distance'], linewidth=2, color='green', alpha=0.8)\n",
        "axes[1, 0].axvline(history['best_epoch'], color='red', linestyle='--', label='Best Epoch')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Wasserstein Distance', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('Wasserstein Distance Over Time', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gradient penalty\n",
        "axes[1, 1].plot(history['gp'], linewidth=2, color='purple', alpha=0.8)\n",
        "axes[1, 1].axvline(history['best_epoch'], color='red', linestyle='--', label='Best Epoch')\n",
        "axes[1, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Gradient Penalty', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_title('Gradient Penalty Over Time', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../plots/improved_ctgan_training.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Training curves saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeutGkD2F-uf"
      },
      "source": [
        "## 6. Generate Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unOkF9BSF-uf"
      },
      "outputs": [],
      "source": [
        "# Generate 5x synthetic data\n",
        "num_real_samples = len(train_data)\n",
        "num_synthetic_samples = 5 * num_real_samples\n",
        "\n",
        "print(f'Generating {num_synthetic_samples:,} synthetic samples (5x augmentation)...')\n",
        "\n",
        "# Generate in transformed space\n",
        "synthetic_transformed = improved_ctgan.generate_samples(num_synthetic_samples)\n",
        "\n",
        "# Inverse transform to original scale\n",
        "synthetic_data = data_transformer.inverse_transform(synthetic_transformed)\n",
        "\n",
        "# Convert to DataFrame\n",
        "synthetic_df = pd.DataFrame(synthetic_data, columns=column_names)\n",
        "\n",
        "print(f'\\n✓ Synthetic data generated!')\n",
        "print(f'Shape: {synthetic_df.shape}')\n",
        "print(f'\\nFirst few rows:')\n",
        "print(synthetic_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kniNrxjSF-uf"
      },
      "source": [
        "## 7. Comprehensive Quality Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d85VzaoMF-uf"
      },
      "outputs": [],
      "source": [
        "# Validate synthetic data quality\n",
        "print('Validating synthetic data quality...')\n",
        "\n",
        "validation_results = validate_synthetic_quality(\n",
        "    real_data=train_array,\n",
        "    synthetic_data=synthetic_data,\n",
        "    column_names=column_names,\n",
        "    continuous_cols=data_transformer.continuous_columns,\n",
        "    binary_cols=data_transformer.binary_columns,\n",
        "    onehot_groups=data_transformer.onehot_groups\n",
        ")\n",
        "\n",
        "# Print detailed report\n",
        "print_validation_report(validation_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPXxvlLvF-ug"
      },
      "source": [
        "## 8. Visualize Distribution Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gHFsYRhF-ug"
      },
      "outputs": [],
      "source": [
        "# Plot distributions for continuous features\n",
        "continuous_cols = data_transformer.continuous_columns\n",
        "n_plots = min(6, len(continuous_cols))\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(n_plots):\n",
        "    col_idx = continuous_cols[i]\n",
        "    col_name = column_names[col_idx]\n",
        "\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Plot histograms\n",
        "    ax.hist(train_data.iloc[:, col_idx], bins=30, alpha=0.6, label='Real',\n",
        "            color='blue', density=True, edgecolor='black')\n",
        "    ax.hist(synthetic_df.iloc[:, col_idx], bins=30, alpha=0.6, label='Synthetic',\n",
        "            color='red', density=True, edgecolor='black')\n",
        "\n",
        "    # Get validation result\n",
        "    result = next((r for r in validation_results['continuous'] if r['column'] == col_name), None)\n",
        "    if result:\n",
        "        status = '✓ PASS' if result['passed'] else '✗ FAIL'\n",
        "        ax.set_title(f\"{col_name}\\n{status} (p={result['ks_p_value']:.4f})\",\n",
        "                    fontsize=11, fontweight='bold')\n",
        "    else:\n",
        "        ax.set_title(col_name, fontsize=11, fontweight='bold')\n",
        "\n",
        "    ax.set_xlabel('Value', fontsize=10)\n",
        "    ax.set_ylabel('Density', fontsize=10)\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../plots/improved_ctgan_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Distribution comparison saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv91ImjEF-uh"
      },
      "source": [
        "## 9. Validate One-Hot Encoded Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJKaYpLF-uh"
      },
      "outputs": [],
      "source": [
        "# Check one-hot validity\n",
        "print('Checking one-hot encoded features...')\n",
        "\n",
        "for group_idx, group in enumerate(data_transformer.onehot_groups):\n",
        "    group_cols = [column_names[i] for i in group]\n",
        "\n",
        "    # Real data\n",
        "    real_group = train_data[group_cols].values\n",
        "    real_sums = np.sum(real_group, axis=1)\n",
        "    real_valid = np.mean(np.isclose(real_sums, 1.0))\n",
        "\n",
        "    # Synthetic data\n",
        "    syn_group = synthetic_df[group_cols].values\n",
        "    syn_sums = np.sum(syn_group, axis=1)\n",
        "    syn_valid = np.mean(np.isclose(syn_sums, 1.0))\n",
        "\n",
        "    print(f'\\nGroup {group_idx}: {group_cols[0].split(\"_\")[0]}')\n",
        "    print(f'  Real validity: {real_valid:.2%}')\n",
        "    print(f'  Synthetic validity: {syn_valid:.2%}')\n",
        "    print(f'  Status: {\"✓ PASS\" if syn_valid > 0.95 else \"✗ FAIL\"}')\n",
        "\n",
        "    # Distribution\n",
        "    print(f'  Category distribution:')\n",
        "    for col in group_cols:\n",
        "        real_freq = train_data[col].mean()\n",
        "        syn_freq = synthetic_df[col].mean()\n",
        "        print(f'    {col}: Real={real_freq:.2%}, Synthetic={syn_freq:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwSh0dHBF-ui"
      },
      "source": [
        "## 10. Save Models and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gX6XliF-ui"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "os.makedirs('../models/improved', exist_ok=True)\n",
        "\n",
        "# Save using pickle (more reliable)\n",
        "with open('../models/improved/improved_ctgan.pkl', 'wb') as f:\n",
        "    pickle.dump(improved_ctgan, f)\n",
        "\n",
        "with open('../models/improved/data_transformer.pkl', 'wb') as f:\n",
        "    pickle.dump(data_transformer, f)\n",
        "\n",
        "synthetic_df.to_csv('../models/improved/synthetic_data.csv', index=False)\n",
        "\n",
        "with open('../models/improved/training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history, f)\n",
        "\n",
        "with open('../models/improved/validation_results.pkl', 'wb') as f:\n",
        "    pickle.dump(validation_results, f)\n",
        "\n",
        "print('✓ All models and data saved successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ACq8dASF-uj"
      },
      "source": [
        "## 11. Summary & Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2OcRcGcF-uj"
      },
      "outputs": [],
      "source": [
        "# Load original CTGAN results for comparison\n",
        "try:\n",
        "    original_ks_results = pd.read_csv('../models/ks_test_results.csv')\n",
        "    original_pass_rate = (original_ks_results['P_Value'] > 0.05).sum() / len(original_ks_results) * 100\n",
        "\n",
        "    print('\\n' + '='*80)\n",
        "    print('COMPARISON: ORIGINAL CTGAN vs IMPROVED CTGAN')\n",
        "    print('='*80)\n",
        "    print(f'\\nOriginal CTGAN:')\n",
        "    print(f'  Overall pass rate: {original_pass_rate:.1f}%')\n",
        "    print(f'  Training epochs: 100')\n",
        "    print(f'  Architecture: Simple (256x256)')\n",
        "    print(f'\\nImproved CTGAN:')\n",
        "    print(f'  Overall pass rate: {validation_results[\"summary\"][\"overall_pass_rate\"]:.1f}%')\n",
        "    print(f'  Training epochs: {len(history[\"g_loss\"])}')\n",
        "    print(f'  Architecture: Advanced (512x512x512)')\n",
        "    print(f'\\nImprovement:')\n",
        "    improvement = validation_results['summary']['overall_pass_rate'] - original_pass_rate\n",
        "    print(f'  Pass rate improvement: {improvement:+.1f}%')\n",
        "    print(f'  Relative improvement: {improvement / original_pass_rate * 100:.1f}%')\n",
        "    print('='*80)\n",
        "except:\n",
        "    print('\\nCould not load original results for comparison')\n",
        "\n",
        "# Summary\n",
        "print('\\n' + '='*80)\n",
        "print('IMPROVED CTGAN TRAINING SUMMARY')\n",
        "print('='*80)\n",
        "print(f'\\nDataset:')\n",
        "print(f'  Real samples: {num_real_samples:,}')\n",
        "print(f'  Synthetic samples: {num_synthetic_samples:,}')\n",
        "print(f'  Augmentation factor: 5.0x')\n",
        "print(f'\\nQuality Metrics:')\n",
        "print(f'  Continuous features: {validation_results[\"summary\"][\"continuous_pass_rate\"]:.1f}%')\n",
        "print(f'  Binary features: {validation_results[\"summary\"][\"binary_pass_rate\"]:.1f}%')\n",
        "print(f'  One-hot groups: {validation_results[\"summary\"][\"onehot_pass_rate\"]:.1f}%')\n",
        "print(f'  Overall quality: {validation_results[\"summary\"][\"overall_pass_rate\"]:.1f}%')\n",
        "print(f'\\nTraining:')\n",
        "print(f'  Epochs completed: {len(history[\"g_loss\"])}')\n",
        "print(f'  Best epoch: {history[\"best_epoch\"]}')\n",
        "print(f'  Final Wasserstein distance: {history[\"w_distance\"][-1]:.4f}')\n",
        "print('='*80)\n",
        "print('\\n✓ Improved CTGAN training complete!')\n",
        "print('Next: Run augmented model evaluation with improved synthetic data')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}